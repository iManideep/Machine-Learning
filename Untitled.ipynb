{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a3daca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "import pydeequ\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b4249c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('train.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d9f66ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- Survived: string (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- SibSp: string (nullable = true)\n",
      " |-- Parch: string (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: string (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2ae2427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydeequ.profiles import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cac63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ColumnProfilerRunner(spark) \\\n",
    "            .onData(df) \\\n",
    "            .run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75014fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pydeequ.profiles.StandardColumnProfile"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.columnProfileClasses['StandardColumnProfile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3ece520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pydeequ.profiles.NumericColumnProfile"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.columnProfileClasses['NumericColumnProfile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2db17b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'PassengerId'\n",
      "\t completeness: 1.0\n",
      "\t approximate number of distinct values: 888\n",
      "\t datatype: Integral\n",
      "\t minimum: 1.0\n",
      "\t maximum: 891.0\n",
      "\t mean: 446.0\n",
      "\t standard deviation: 257.20938292890224\n",
      "Column 'Name'\n",
      "\t completeness: 1.0\n",
      "\t approximate number of distinct values: 936\n",
      "\t datatype: String\n",
      "Column 'Ticket'\n",
      "\t completeness: 1.0\n",
      "\t approximate number of distinct values: 710\n",
      "\t datatype: String\n",
      "Column 'Pclass'\n",
      "\t completeness: 1.0\n",
      "\t approximate number of distinct values: 3\n",
      "\t datatype: Integral\n",
      "\t minimum: 1.0\n",
      "\t maximum: 3.0\n",
      "\t mean: 2.308641975308642\n",
      "\t standard deviation: 0.8356019334795166\n",
      "Column 'Parch'\n",
      "\t completeness: 1.0\n",
      "\t approximate number of distinct values: 7\n",
      "\t datatype: Integral\n",
      "\t minimum: 0.0\n",
      "\t maximum: 6.0\n",
      "\t mean: 0.38159371492704824\n",
      "\t standard deviation: 0.8056047612452213\n",
      "Column 'Embarked'\n",
      "\t completeness: 0.9977553310886644\n",
      "\t approximate number of distinct values: 3\n",
      "\t datatype: String\n",
      "Column 'Age'\n",
      "\t completeness: 0.8013468013468014\n",
      "\t approximate number of distinct values: 83\n",
      "\t datatype: Fractional\n",
      "\t minimum: 0.42\n",
      "\t maximum: 80.0\n",
      "\t mean: 29.69911764705882\n",
      "\t standard deviation: 14.51632115081731\n",
      "Column 'Cabin'\n",
      "\t completeness: 0.22895622895622897\n",
      "\t approximate number of distinct values: 149\n",
      "\t datatype: String\n",
      "Column 'Fare'\n",
      "\t completeness: 1.0\n",
      "\t approximate number of distinct values: 241\n",
      "\t datatype: Fractional\n",
      "\t minimum: 0.0\n",
      "\t maximum: 512.3292\n",
      "\t mean: 32.2042079685746\n",
      "\t standard deviation: 49.6655344447741\n",
      "Column 'SibSp'\n",
      "\t completeness: 1.0\n",
      "\t approximate number of distinct values: 7\n",
      "\t datatype: Integral\n",
      "\t minimum: 0.0\n",
      "\t maximum: 8.0\n",
      "\t mean: 0.5230078563411896\n",
      "\t standard deviation: 1.1021244350892876\n",
      "Column 'Survived'\n",
      "\t completeness: 1.0\n",
      "\t approximate number of distinct values: 2\n",
      "\t datatype: Integral\n",
      "\t minimum: 0.0\n",
      "\t maximum: 1.0\n",
      "\t mean: 0.3838383838383838\n",
      "\t standard deviation: 0.4863193178670999\n",
      "Column 'Sex'\n",
      "\t completeness: 1.0\n",
      "\t approximate number of distinct values: 2\n",
      "\t datatype: String\n"
     ]
    }
   ],
   "source": [
    "for col, profile in result.profiles.items():\n",
    "    print(f'Column \\'{col}\\'')\n",
    "    print('\\t',f'completeness: {profile.completeness}')\n",
    "    print('\\t',f'approximate number of distinct values: {profile.approximateNumDistinctValues}')\n",
    "    print('\\t',f'datatype: {profile.dataType}')\n",
    "    if profile.dataType == 'Integral' or profile.dataType == 'Fractional':\n",
    "        print('\\t',f\"minimum: {profile.minimum}\")\n",
    "        print('\\t',f\"maximum: {profile.maximum}\")\n",
    "        print('\\t',f\"mean: {profile.mean}\")\n",
    "        print('\\t',f\"standard deviation: {profile.stdDev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acddfecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    entity  instance                        name  value\n",
      "0   Column  Survived                Completeness    1.0\n",
      "1   Column  Survived              Histogram.bins    5.0\n",
      "2   Column  Survived       Histogram.abs.Boolean    0.0\n",
      "3   Column  Survived     Histogram.ratio.Boolean    0.0\n",
      "4   Column  Survived    Histogram.abs.Fractional    0.0\n",
      "5   Column  Survived  Histogram.ratio.Fractional    0.0\n",
      "6   Column  Survived      Histogram.abs.Integral  891.0\n",
      "7   Column  Survived    Histogram.ratio.Integral    1.0\n",
      "8   Column  Survived       Histogram.abs.Unknown    0.0\n",
      "9   Column  Survived     Histogram.ratio.Unknown    0.0\n",
      "10  Column  Survived        Histogram.abs.String    0.0\n",
      "11  Column  Survived      Histogram.ratio.String    0.0\n",
      "12  Column  Survived               CountDistinct    2.0\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.analyzers import *\n",
    "\n",
    "\n",
    "analysisResult = AnalysisRunner(spark).onData(df)\n",
    "analysisResult= analysisResult.addAnalyzer(Completeness('Survived'))\n",
    "analysisResult= analysisResult.addAnalyzer(CountDistinct('Survived'))\n",
    "analysisResult= analysisResult.addAnalyzer(Maximum('Survived'))\n",
    "analysisResult= analysisResult.addAnalyzer(DataType('Survived'))\n",
    "analysisResult= analysisResult.run()\n",
    "a_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult, pandas = True)\n",
    "print(a_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a6b29d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4360beaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,concat,lit\n",
    "import os\n",
    "from pydeequ.analyzers import Mean,Maximum\n",
    "import pydeequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b50d5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').option('header','true').load('Data_Profile.trg')\n",
    "source_name_file_prefix_list = df.select('source_name_file_prefix').collect()\n",
    "source_name_file_prefix_list = [i.source_name_file_prefix for i in source_name_file_prefix_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15f49ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "landing_zone_files = os.listdir()\n",
    "feed_files = dict()\n",
    "for i in source_name_file_prefix_list:\n",
    "    feed_files[i] = [file_name for file_name in landing_zone_files if file_name.startswith(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14951d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').option('header','true').load('Data_Profile_Config.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d7cc277",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\"*\",concat(col(\"Source\"),lit('_'),col('File Prefix')).alias(\"source_name_file_prefix\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45bf8348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------+-------------------+\n",
      "|entity|instance|         name|              value|\n",
      "+------+--------+-------------+-------------------+\n",
      "|Column|   Cabin| Completeness|0.21658986175115208|\n",
      "|Column|     Sex|CountDistinct|                2.0|\n",
      "|Column|     Age| Completeness| 0.7995391705069125|\n",
      "|Column|Survived|CountDistinct|                2.0|\n",
      "+------+--------+-------------+-------------------+\n",
      "\n",
      "+------+--------+-------------+-------------------+\n",
      "|entity|instance|         name|              value|\n",
      "+------+--------+-------------+-------------------+\n",
      "|Column|   Cabin| Completeness|0.24070021881838075|\n",
      "|Column|     Sex|CountDistinct|                2.0|\n",
      "|Column|     Age| Completeness| 0.8030634573304157|\n",
      "|Column|Survived|CountDistinct|                2.0|\n",
      "+------+--------+-------------+-------------------+\n",
      "\n",
      "+-------+--------+-------------+-----+\n",
      "| entity|instance|         name|value|\n",
      "+-------+--------+-------------+-----+\n",
      "| Column| Species|CountDistinct|  1.0|\n",
      "|Dataset|       *|         Size| 50.0|\n",
      "+-------+--------+-------------+-----+\n",
      "\n",
      "+-------+--------+-------------+-----+\n",
      "| entity|instance|         name|value|\n",
      "+-------+--------+-------------+-----+\n",
      "| Column| Species|CountDistinct|  2.0|\n",
      "|Dataset|       *|         Size|100.0|\n",
      "+-------+--------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in feed_files:\n",
    "    list_of_analyzers = df.where(df.source_name_file_prefix == i).rdd.collect()\n",
    "    for file in feed_files[i]:\n",
    "        feed_df = spark.read.format('csv').option('header','true').load(file)\n",
    "        analysisResult = AnalysisRunner(spark).onData(feed_df)\n",
    "        for row in list_of_analyzers:\n",
    "            if row['Column Name'] is not None:\n",
    "                list_of_columns = list(map(lambda x:x.strip(),row['Column Name'].split(',')))\n",
    "                for i in list_of_columns:\n",
    "                    if row['Profile Function'] == \"Completeness\":\n",
    "                        analysisResult = analysisResult.addAnalyzer(Completeness(i))\n",
    "                    elif row['Profile Function'] == \"CountDistinct\":\n",
    "                        analysisResult = analysisResult.addAnalyzer(CountDistinct(i))\n",
    "                    elif row['Profile Function'] == \"DataType\":\n",
    "                        analysisResult = analysisResult.addAnalyzer(DataType(i))\n",
    "                    elif row['Profile Function'] == \"Maximum\":\n",
    "                        analysisResult = analysisResult.addAnalyzer(Maximum(i))\n",
    "                    elif row['Profile Function'] == \"Mean\":\n",
    "                        analysisResult = analysisResult.addAnalyzer(Mean(i))\n",
    "            else:\n",
    "                list_of_columns = None\n",
    "                if row['Profile Function'] == \"Size\":\n",
    "                    analysisResult = analysisResult.addAnalyzer(Size())\n",
    "        analysisResult = analysisResult.run()\n",
    "        analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)\n",
    "        analysisResult_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502a3eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
